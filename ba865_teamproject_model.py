# -*- coding: utf-8 -*-
"""BA865_teamproject_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gk2JCYN5T1beKpZCUi8k3CGJ05vnDmGO
"""

# Import Necessary Libraries
import time
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPooling2D , Flatten , Dropout , BatchNormalization, GlobalAveragePooling2D
from keras.optimizers import SGD, Adam

"""## **Model building and training**"""

# Model with ReLU
from keras.layers import ReLU

def main():
  model = Sequential()

  model.add(Conv2D(32, kernel_size=3, padding='same', input_shape=(224,224,3)))
  model.add(ReLU())
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Conv2D(64, kernel_size=3, padding='same'))
  model.add(ReLU())
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Conv2D(128, kernel_size=5, padding='same'))
  model.add(ReLU())
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Flatten())
  model.add(Dense(num_classes,activation='softmax'))

  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  start_time = time.time() # Start time

  model.fit(train_generator,steps_per_epoch=train_generator.samples // train_generator.batch_size,epochs=10)

  end_time = time.time() # End time
  duration = end_time - start_time # Duration
  print(f"Training the model took {duration:.2f} seconds.")

  # Evaluate the model
  start_time = time.time() # Start time
  test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)
  print("Test Accuracy:", test_acc * 100 , "%")
  print('Test Loss:',test_loss)
  end_time = time.time() # End time
  duration = end_time - start_time # Duration
  print(f"Evaluating the model took {duration:.2f} seconds.")

  print("========================================")
  y_pred = np.argmax(model.predict(test_generator), axis=1)
  y_true = test_generator.classes
  f1 = f1_score(y_true, y_pred, average='weighted')
  cm = confusion_matrix(y_true, y_pred)
  print(f"F1 Score: {f1}")
  # print(f"Confusion Matrix: {cm}")
  print("========================================")
  plt.figure(figsize=(10, 8))
  sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
  plt.xlabel('Predicted labels')
  plt.ylabel('True labels')
  plt.title('Confusion Matrix')
  plt.show()

main()

# Model with LeakyReLU
from keras.layers import LeakyReLU

def main():
  model = Sequential()

  model.add(Conv2D(32, kernel_size=3, padding='same', input_shape=(224,224,3)))
  model.add(LeakyReLU(alpha=0.1))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Conv2D(64, kernel_size=3, padding='same'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Conv2D(128, kernel_size=5, padding='same'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))


  model.add(Flatten())
  model.add(Dense(num_classes,activation='softmax'))

  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  start_time = time.time() # Start time

  model.fit(train_generator,steps_per_epoch=train_generator.samples // train_generator.batch_size,epochs=10)

  end_time = time.time() # End time
  duration = end_time - start_time # Duration
  print(f"Training the model took {duration:.2f} seconds.")

  # Evaluate the model
  start_time = time.time() # Start time
  test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)
  print("Test Accuracy:", test_acc * 100 , "%")
  print('Test Loss:',test_loss)
  end_time = time.time() # End time
  duration = end_time - start_time # Duration
  print(f"Evaluating the model took {duration:.2f} seconds.")

  print("========================================")
  y_pred = np.argmax(model.predict(test_generator), axis=1)
  y_true = test_generator.classes
  f1 = f1_score(y_true, y_pred, average='weighted')
  cm = confusion_matrix(y_true, y_pred)
  print(f"F1 Score: {f1}")
  # print(f"Confusion Matrix: {cm}")
  print("========================================")
  plt.figure(figsize=(10, 8))
  sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
  plt.xlabel('Predicted labels')
  plt.ylabel('True labels')
  plt.title('Confusion Matrix')
  plt.show()

main()

# Model with ELU
from keras.layers import ELU


def main():
  model = Sequential()

  model.add(Conv2D(32, kernel_size=3, padding='same', input_shape=(224,224,3)))
  model.add(ELU())
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Conv2D(64, kernel_size=3, padding='same'))
  model.add(ELU())
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))

  model.add(Conv2D(128, kernel_size=5, padding='same'))
  model.add(ELU())
  model.add(BatchNormalization())
  model.add(MaxPooling2D(2))
  model.add(Dropout(0.2))


  model.add(Flatten())
  model.add(Dense(num_classes,activation='softmax'))

  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  start_time = time.time() # Start time

  model.fit(train_generator,steps_per_epoch=train_generator.samples // train_generator.batch_size,epochs=10)

  end_time = time.time() # End time
  duration = end_time - start_time # Duration
  print(f"Training the model took {duration:.2f} seconds.")

  # Evaluate the model
  start_time = time.time() # Start time
  test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)
  print("Test Accuracy:", test_acc * 100 , "%")
  print('Test Loss:',test_loss)
  end_time = time.time() # End time
  duration = end_time - start_time # Duration
  print(f"Evaluating the model took {duration:.2f} seconds.")

  print("========================================")
  y_pred = np.argmax(model.predict(test_generator), axis=1)
  y_true = test_generator.classes
  f1 = f1_score(y_true, y_pred, average='weighted')
  cm = confusion_matrix(y_true, y_pred)
  print(f"F1 Score: {f1}")
  # print(f"Confusion Matrix: {cm}")
  print("========================================")
  plt.figure(figsize=(10, 8))
  sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
  plt.xlabel('Predicted labels')
  plt.ylabel('True labels')
  plt.title('Confusion Matrix')
  plt.show()

main()

"""# **Hyper-parameter tuning through WandB**"""

!pip install wandb

# Import Necessary Libraries
import numpy as np
import tensorflow as tf
from sklearn.metrics import confusion_matrix, f1_score
import wandb
from wandb.keras import WandbCallback

# Log in to WandB
wandb.login()

# Define Callback class
class MetricsCallback(tf.keras.callbacks.Callback):
    def __init__(self, test_data):
        self.test_data = test_data

    def on_epoch_end(self, epoch, logs=None):
        x, y_true = self.test_data
        y_pred = np.argmax(self.model.predict(x), axis=1)
        cm = confusion_matrix(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='macro')
        wandb.log({"F1 Score": f1, "Confusion Matrix": wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=y_pred)})

def main():
    # Initialize WandB
    wandb.init()
    # Initialize config
    config = wandb.config

    model = Sequential()

    model.add(Conv2D(32, kernel_size=3, padding='same', input_shape=(224,224,3)))
    model.add(ELU(alpha=1.0))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(2))
    model.add(Dropout(config.drop_out))

    model.add(Conv2D(64, kernel_size=3, padding='same'))
    model.add(ELU(alpha=1.0))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(2))
    model.add(Dropout(config.drop_out))

    model.add(Conv2D(128, kernel_size=5, padding='same'))
    model.add(ELU(alpha=1.0))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(2))
    model.add(Dropout(config.drop_out))

    model.add(Flatten())
    model.add(Dense(config.num_classes, activation='softmax'))

    # Compile the model with the specified learning rate
    optimizer = Adam(learning_rate=config.learning_rate)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    start_time = time.time() # Start time

    model.fit(train_generator, epochs=10, callbacks=[WandbCallback(), MetricsCallback((test_generator, test_generator.classes))])

    end_time = time.time() # End time
    duration = end_time - start_time # Duration
    print(f"Training the model took {duration:.2f} seconds.")

    # Evaluate the model
    start_time = time.time() # Start time
    test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)
    wandb.log({"Test Accuracy": test_acc, "Test Loss": test_loss})
    print("Test Accuracy:", test_acc * 100 , "%")
    print('Test Loss:',test_loss)
    end_time = time.time() # End time
    duration = end_time - start_time # Duration
    print(f"Evaluating the model took {duration:.2f} seconds.")

# Define the sweep configuration
sweep_config = {
    'method': 'grid',
    'parameters': {
        'drop_out': {
            'values': [0.2, 0.3]
        },
        'learning_rate': {
            'values': [0.01, 0.001]
        },
        'num_classes': {
            'values': [num_classes]
        },
        'batch_size': {
            'values': [32, 64]
        }
    }
}

# Initialize sweep by passing in the config
sweep_id = wandb.sweep(sweep_config, project="BA865_Gesture_Recognition_Final")

# Start sweep job
wandb.agent(sweep_id, function=main)

# Finish sweep job
wandb.finish()